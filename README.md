# ğŸ§  Neural Network from Scratch in C++ â€” L7 Capstone Project
[![C++](https://img.shields.io/badge/C%2B%2B-17-blue.svg)](https://isocpp.org/)
[![Build](https://github.com/Trojan3877/neural-network-from-scratch-cpp/actions/workflows/ci.yml/badge.svg)](https://github.com/Trojan3877/neural-network-from-scratch-cpp/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Linux%20%7C%20macOS%20%7C%20Windows-lightgrey.svg)]()
[![Capstone](https://img.shields.io/badge/Capstone-L7%20Quality-purple.svg)]()
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)]()
[![Stars](https://img.shields.io/github/stars/Trojan3877/neural-network-from-scratch-cpp.svg?style=social)](https://github.com/Trojan3877/neural-network-from-scratch-cpp/stargazers)
[![Forks](https://img.shields.io/github/forks/Trojan3877/neural-network-from-scratch-cpp.svg?style=social)](https://github.com/Trojan3877/neural-network-from-scratch-cpp/network/members)

A **production-grade neural network framework built entirely from scratch in modern C++**.  
This project demonstrates both **machine learning fundamentals** and **high-quality C++ systems engineering**, designed to be **capstone-ready** and attractive to **Big Tech / Big AI recruiters**.

> ğŸ¯ Focus: Deep understanding of neural networks, backpropagation, and scalable software design â€” without relying on ML libraries.

---

## ğŸš€ Key Features

âœ… From-scratch feedforward neural network  
âœ… Modular layers, activations, and loss functions  
âœ… Backpropagation + SGD training loop  
âœ… CSV dataset loader  
âœ… Modern **CMake** build system  
âœ… **GoogleTest** unit testing  
âœ… **CI/CD with GitHub Actions**  
âœ… Benchmarks & metrics  
âœ… Clean, extensible API  
âœ… Visual architecture flowchart  
âœ… Cross-platform (Linux / macOS / Windows)

---

## ğŸ§ª Tech Stack

- **Language:** C++17 (Modern C++)
- **Build:** CMake
- **Testing:** GoogleTest
- **CI/CD:** GitHub Actions
- **Math:** Custom implementation (Eigen-ready)
- **Data:** CSV loader
- **Formatting:** clang-format
- **Platforms:** Linux, macOS, Windows

---

## ğŸ“ Project Structure
neural-network-from-scratch-cpp/ â”œâ”€â”€ CMakeLists.txt â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ core/ â”‚   â”‚   â”œâ”€â”€ NeuralNet.hpp â”‚   â”‚   â”œâ”€â”€ Layer.hpp â”‚   â”‚   â”œâ”€â”€ Activation.hpp â”‚   â”‚   â””â”€â”€ Loss.hpp â”‚   â”œâ”€â”€ impl/ â”‚   â”‚   â”œâ”€â”€ DenseLayer.cpp â”‚   â”‚   â”œâ”€â”€ Activations.cpp â”‚   â”‚   â”œâ”€â”€ Losses.cpp â”‚   â”‚   â””â”€â”€ CSVLoader.cpp â”‚   â””â”€â”€ main.cpp â”œâ”€â”€ include/ â”œâ”€â”€ tests/ â”‚   â”œâ”€â”€ test_network.cpp â”‚   â””â”€â”€ test_activation.cpp â”œâ”€â”€ examples/ â”‚   â”œâ”€â”€ synthetic_dataset.csv â”‚   â””â”€â”€ mnist_example.cpp â”œâ”€â”€ docs/ â”‚   â”œâ”€â”€ architecture.png â”‚   â””â”€â”€ benchmarks.md â”œâ”€â”€ .github/workflows/ci.yml â”œâ”€â”€ LICENSE â””â”€â”€ README.md
---

## âš¡ Quick Start

### 1ï¸âƒ£ Clone the repo

```bash
git clone https://github.com/Trojan3877/neural-network-from-scratch-cpp.git
cd neural-network-from-scratch-cpp
2ï¸âƒ£ Build with CMake
Copy code
Bash
mkdir build && cd build
cmake ..
cmake --build .
3ï¸âƒ£ Run demo
Copy code
Bash
./nn_demo ../examples/synthetic_dataset.csv
ğŸ§  Architecture Overview
Input â†’ Dense â†’ Activation â†’ Dense â†’ Activation â†’ Output
           â†‘                 â†“
        Backpropagation & Gradient Updates
ğŸ“Š Benchmarks & Metrics
Dataset
Accuracy
Epochs
Time (s)
Synthetic CSV
92.4%
100
0.8
ğŸ“„ Detailed results:
â¡ï¸ docs/benchmarks.md
Environment:
Compiler: GCC 11 / Clang 15
Flags: -O2
CPU: x86_64
ğŸ§ª Testing
Run all unit tests:
Copy code
Bash
ctest --test-dir build
âœ”ï¸ Layer correctness
âœ”ï¸ Forward / backward pass
âœ”ï¸ Loss convergence
âœ”ï¸ Build stability
ğŸ“ˆ Why This Project Matters
This repository showcases:
ğŸ”¬ Deep ML fundamentals (no black boxes)
ğŸ—ï¸ Strong modern C++ design
ğŸ§ª Test-driven development
âš™ï¸ Build systems & CI/CD
ğŸ“š Capstone-level documentation
ğŸ“Š Quantified results

ğŸ›£ï¸ Roadmap
Planned enhancements:
[ ] CUDA / GPU acceleration
[ ] Eigen / BLAS math backend
[ ] Model save & load
[ ] CLI training interface
[ ] CNN / RNN layers
[ ] Python bindings
[ ] ONNX export
ğŸ“œ License
This project is licensed under the MIT License â€” free to use, modify, and distribute.
Design Questions & Reflections
Q: What problem does this project aim to solve?
A: This project implements a neural network from scratch in C++ to deepen my understanding of how core learning algorithms work at a lower level. Rather than relying on high-level frameworks, I wanted to explore the mechanics of forward passes, loss computation, backpropagation, and weight updates in a language close to the hardware.
Q: Why did I choose to build this from scratch instead of using a library?
A: I chose to build it from scratch because frameworks often abstract away the math and control flow that make neural networks effective. Writing the algorithms myself helped me see the underlying mechanics and trade-offs, and C++ gave me a sense of how performance and memory management interact with model logic.
Q: What were the main trade-offs I made?
A: The main trade-off was implementation complexity versus ease of experimentation. Writing everything manually meant slower development and more boilerplate, but it rewarded me with deep, first-principles understanding that I wouldnâ€™t have gotten by using ready-made libraries.
Q: What didnâ€™t work as expected?
A: In early versions, I struggled to get the gradient calculations correct â€” subtle sign errors and incorrect matrix indexing led to training that didnâ€™t converge. Fixing this forced me to slow down and validate every step, which improved my grasp of backpropagation and numerical stability.
Q: What did I learn from building this project?
A: I learned that many of the behaviors we take for granted in libraries â€” stable gradients, efficient memory layout, numerical precision â€” are the result of intentional design choices. Building from scratch clarified both the why and how of deep learning fundamentals.
Q: If I had more time or resources, what would I improve next?
A: I would add better testing for edge cases and integrate simple visualizations to watch how weight updates evolve during training. Iâ€™d also explore optimizing the code further, exploring SIMD or parallel computation to see how much performance could be gained.
ğŸ™Œ Author
Corey Leath
GitHub: https://github.com/Trojan3877
Aspiring AI/ML Engineer | Building production-ready systems from the ground up to pursue Big Tech & Big AI roles.
â­ If you find this project useful, please consider starring the repo!
